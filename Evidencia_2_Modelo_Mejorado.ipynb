{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "deUepZaUW8y_",
        "U0qmbGqGX5eN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariadna-guzman/E3-Deteccion-de-plagio-TC3002B.301/blob/main/Evidencia_2_Modelo_Mejorado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evidencia 2: Modelo Mejorado\n",
        "\n",
        "- Ariadna Jocelyn Guzmán Jiménez A01749373\n",
        "- Jorge Chávez Badillo A01749448\n",
        "- Amy Murakami Tsutsumi A01750185\n"
      ],
      "metadata": {
        "id": "eztatpdFI1Lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación de Librerías"
      ],
      "metadata": {
        "id": "vdAvGXBgsLUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWJURrVQsPEh",
        "outputId": "2c764cd5-eabe-488f-c161-ffde1ad34824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentencepiece"
      ],
      "metadata": {
        "id": "1jM8BLc3sVsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langdetect"
      ],
      "metadata": {
        "id": "1Lhyx6ZHsarO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TasqTcpctiCR",
        "outputId": "b61cf79c-3c86-42f9-a4b0-c9caa3e0f986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de Librerías"
      ],
      "metadata": {
        "id": "AufBG2q7JsUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kit de herramientas de lenguaje natural\n",
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "import torch\n",
        "# Para el cálculo de la distancia entre párrafos\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Para la manipulación y lectura de archivos\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "# Para la visualización, análisis y manipulación de los datos \n",
        "import pandas as pd\n",
        "from tqdm import tqdm \n",
        "# Para la creación de vectores y matrices\n",
        "import numpy as np\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "# Implementación de BERT\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
        "# Para la traducción de textos\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from langdetect import detect, DetectorFactory"
      ],
      "metadata": {
        "id": "YavA4LIGJxP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de Textos"
      ],
      "metadata": {
        "id": "hzt_3cvM-ZsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AmacCR_Y9h4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88855024-397f-426d-ece5-ee4203d5d04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genuinos = '/content/drive/Shareddrives/Desarrollo de Aplicaciones Avanzadas de Ciencias Computacionales/Reto/Evidencia 2/2023-Datasets/Etapa1/documentos-genuinos/'\n",
        "sospechosos = '/content/drive/Shareddrives/Desarrollo de Aplicaciones Avanzadas de Ciencias Computacionales/Reto/Evidencia 2/2023-Datasets/Etapa1/docmentos-sospechosos/'"
      ],
      "metadata": {
        "id": "JhEAJoNT-kQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de Data Frame"
      ],
      "metadata": {
        "id": "94puq6xMxB4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_frame(path):\n",
        "  content = []\n",
        "  names = []\n",
        "  files_list = sorted(os.listdir(path))\n",
        "  for file_name in files_list:\n",
        "    with open(path + file_name, 'r') as file:\n",
        "      data = file.read().rstrip()\n",
        "      content.append(data)\n",
        "      names.append(file_name)\n",
        "  dictionary = {'name': names, 'content': content}\n",
        "  df = pd.DataFrame(dictionary)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Leh2eZFezfSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_genuinos = create_frame(genuinos)\n",
        "df_genuinos.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "dRuWX0T52IX-",
        "outputId": "987f8841-40c2-4ad6-9922-422f0301c65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          name                                            content\n",
              "0  org-300.txt  Automatic software plagiarism detection tools ...\n",
              "1  org-301.txt  A casual comment by a student alerted the auth...\n",
              "2  org-302.txt  Paraphrase types have been proposed by researc...\n",
              "3  org-303.txt  This paper addresses the issue of text matchin...\n",
              "4  org-304.txt  This work presents a Sentence Hashing Algorith..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e357a9dd-320d-48a6-bc69-13922aaa7d09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>org-300.txt</td>\n",
              "      <td>Automatic software plagiarism detection tools ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>org-301.txt</td>\n",
              "      <td>A casual comment by a student alerted the auth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>org-302.txt</td>\n",
              "      <td>Paraphrase types have been proposed by researc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>org-303.txt</td>\n",
              "      <td>This paper addresses the issue of text matchin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>org-304.txt</td>\n",
              "      <td>This work presents a Sentence Hashing Algorith...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e357a9dd-320d-48a6-bc69-13922aaa7d09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e357a9dd-320d-48a6-bc69-13922aaa7d09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e357a9dd-320d-48a6-bc69-13922aaa7d09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación de Bert\n",
        "***(Bidirectional Encoder Representations From Transformers)***"
      ],
      "metadata": {
        "id": "-Xbh7iK33OIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case = True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           output_attentions = False,\n",
        "                                                           output_hidden_states = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OXfRZlA4BsU",
        "outputId": "fa8312fc-9652-4440-d794-8e9a89c8453f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización"
      ],
      "metadata": {
        "id": "H3iVNj-E5oeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Por texto"
      ],
      "metadata": {
        "id": "NE4ko3g55sTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(tokenizer, model, abstract, MAX_LEN = 510):\n",
        "  inputs = tokenizer.encode(abstract, \n",
        "                            add_special_tokens = True, \n",
        "                            max_length = MAX_LEN,)    \n",
        "\n",
        "  results = pad_sequences([inputs], \n",
        "                          maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", \n",
        "                          truncating=\"post\", \n",
        "                          padding=\"post\")\n",
        "  inputs = results[0] # Eliminar lista exterior\n",
        "\n",
        "  # Máscaras\n",
        "  ''' for i in inputs:\n",
        "    mask = int(i>0)'''\n",
        "  mask = [int(i > 0) for i in inputs]\n",
        "\n",
        "  # Tensores\n",
        "  inputs = torch.tensor(inputs)\n",
        "  mask = torch.tensor(mask)\n",
        "\n",
        "  # Dimensión adicional para batch\n",
        "  inputs = inputs.unsqueeze(0)\n",
        "  mask = mask.unsqueeze(0)\n",
        "\n",
        "  # Evaluación de modelo\n",
        "  model.eval()\n",
        "\n",
        "  # Recopílación de estado a través de BERT\n",
        "  with torch.no_grad():        \n",
        "        logits, encoded_layers = model(input_ids = inputs, \n",
        "                                        token_type_ids = None, \n",
        "                                        attention_mask = mask,\n",
        "                                        return_dict = False)\n",
        "\n",
        "  layer_i = 12 # Última capa BERT antes de clasificación\n",
        "  batch_i = 0 # Entrada del batch\n",
        "  token_i = 0 # Primer token\n",
        "\n",
        "  # Extraer el vector\n",
        "  vector = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "  # Numpy array\n",
        "  vector = vector.detach().cpu().numpy()\n",
        "\n",
        "  return vector"
      ],
      "metadata": {
        "id": "UnOgQOQr56j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base de Datos"
      ],
      "metadata": {
        "id": "ZmaUdLAL51Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_database(data):\n",
        "  vectors = []\n",
        "\n",
        "  # Obtención de datos generales del contenido\n",
        "  source = data.content.values\n",
        "\n",
        "  # Recorrer texto para la obtención de los embeddings\n",
        "  for content in tqdm(source):\n",
        "    # Obtener el embedding\n",
        "    vector = vectorize_text(tokenizer, model, content)\n",
        "    vectors.append(vector)\n",
        "  \n",
        "  data['vectors'] = vectors\n",
        "  data['vectors'] = data['vectors'].apply(lambda emb: np.array(emb))\n",
        "  data['vectors'] = data['vectors'].apply(lambda emb: emb.reshape(1, -1))\n",
        "  return data"
      ],
      "metadata": {
        "id": "9a1RqCPO9o6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de Vector para los Archivos Genuinos"
      ],
      "metadata": {
        "id": "gA_g_mQ8F2kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_genuinos = vectorize_database(df_genuinos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMAeKsuCF2DD",
        "outputId": "4287e499-b6ef-414b-baf6-207f7789a6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [04:00<00:00,  2.00s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_genuinos.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "q3K4i57BMAdo",
        "outputId": "ef436230-9085-4b88-b5f4-ffe37737f807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          name                                            content  \\\n",
              "0  org-300.txt  Automatic software plagiarism detection tools ...   \n",
              "1  org-301.txt  A casual comment by a student alerted the auth...   \n",
              "2  org-302.txt  Paraphrase types have been proposed by researc...   \n",
              "3  org-303.txt  This paper addresses the issue of text matchin...   \n",
              "4  org-304.txt  This work presents a Sentence Hashing Algorith...   \n",
              "\n",
              "                                             vectors  \n",
              "0  [[-0.64271873, -0.26977772, -0.763381, -0.0089...  \n",
              "1  [[-0.51203465, -0.3626315, -0.53556347, 0.1047...  \n",
              "2  [[-0.5537736, -0.32080397, -0.92781377, -0.348...  \n",
              "3  [[-1.0950634, -0.34306043, -0.30031648, 0.2014...  \n",
              "4  [[-0.9064658, -0.37186188, -0.015655695, -0.31...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d89fc8a-bb23-43e8-af2e-6d0015902f93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>content</th>\n",
              "      <th>vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>org-300.txt</td>\n",
              "      <td>Automatic software plagiarism detection tools ...</td>\n",
              "      <td>[[-0.64271873, -0.26977772, -0.763381, -0.0089...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>org-301.txt</td>\n",
              "      <td>A casual comment by a student alerted the auth...</td>\n",
              "      <td>[[-0.51203465, -0.3626315, -0.53556347, 0.1047...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>org-302.txt</td>\n",
              "      <td>Paraphrase types have been proposed by researc...</td>\n",
              "      <td>[[-0.5537736, -0.32080397, -0.92781377, -0.348...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>org-303.txt</td>\n",
              "      <td>This paper addresses the issue of text matchin...</td>\n",
              "      <td>[[-1.0950634, -0.34306043, -0.30031648, 0.2014...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>org-304.txt</td>\n",
              "      <td>This work presents a Sentence Hashing Algorith...</td>\n",
              "      <td>[[-0.9064658, -0.37186188, -0.015655695, -0.31...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d89fc8a-bb23-43e8-af2e-6d0015902f93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d89fc8a-bb23-43e8-af2e-6d0015902f93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d89fc8a-bb23-43e8-af2e-6d0015902f93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones para la Detección de Plagio"
      ],
      "metadata": {
        "id": "vHMkOllIKGvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_cosine(sus_text):\n",
        "  '''\n",
        "  Función que crea un vector de acuerdo al texto de entrada \n",
        "  para ajustarlo para calcular la similitud de coseno.\n",
        "  '''\n",
        "  \n",
        "  sus_vect = vectorize_text(tokenizer, model, sus_text)\n",
        "  sus_vect = np.array(sus_vect)\n",
        "  sus_vect = sus_vect.reshape(1, -1)\n",
        "  return sus_vect"
      ],
      "metadata": {
        "id": "szyO5FcsKOAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_plagiarism(score, threshold):\n",
        "  '''\n",
        "  Función que determina si el texto es plagio o no de acuerdo \n",
        "  a un cierto umbral.\n",
        "  '''\n",
        "\n",
        "  status = False\n",
        "\n",
        "  if(score >= threshold):\n",
        "    status = True\n",
        "  \n",
        "  return status "
      ],
      "metadata": {
        "id": "gJe8VKwELrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plagiarism_analysis(sus_text, database_vector, threshold):\n",
        "  '''\n",
        "  Función que regresa los resultados de la detección de plagio, señalando\n",
        "  los archivos comparados y el nivel de similitud obtenido.\n",
        "  '''\n",
        "\n",
        "  sus_vect = vectorize_cosine(sus_text)\n",
        "\n",
        "  # Similitud Coseno\n",
        "  database_vector['similarity'] = database_vector['vectors'].apply(\n",
        "      lambda x: cosine_similarity(sus_vect, x))\n",
        "  database_vector['similarity'] = database_vector['similarity'].apply(\n",
        "      lambda x: x[0][0])\n",
        "  \n",
        " # Top 5 Archivos similares  \n",
        "  similar_files = database_vector.sort_values(by = 'similarity', \n",
        "                                              ascending = False)[0:4]\n",
        "  df_result = similar_files[['name', \n",
        "                             'content', \n",
        "                             'similarity']].reset_index(drop = True)\n",
        "  \n",
        "  similarity_score = df_result.iloc[0]['similarity']\n",
        "  most_similar_file = df_result.iloc[0]['content']\n",
        "  status = detect_plagiarism(similarity_score, threshold)\n",
        "\n",
        "  plagiarism_analysis = ('Archivo Sospechoso: \\n' \n",
        "                         + sus_text\n",
        "                         + '\\nStatus: ' \n",
        "                         + str(status) \n",
        "                         + '\\nSimilitud: ' \n",
        "                         + str(similarity_score) \n",
        "                         + '\\nArchivo Similar: ' \n",
        "                         + most_similar_file)\n",
        "  return plagiarism_analysis"
      ],
      "metadata": {
        "id": "GSdA_pibNKcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detección de Plagio"
      ],
      "metadata": {
        "id": "t62Tlc8yS2e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sus = create_frame(sospechosos)\n",
        "df_sus.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "THST2Q9gS52_",
        "outputId": "1d32aef5-e9c7-4366-fb7c-89c1fae18702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         name                                            content\n",
              "0  FID-01.txt  This systematic review provides unique finding...\n",
              "1  FID-02.txt  In this review, we describe the application of...\n",
              "2  FID-03.txt  How to measure similarity between two sequence...\n",
              "3  FID-04.txt  Text similarity measurement is the basis of na...\n",
              "4  FID-05.txt  Hoy en día el idioma ha dejado de ser una barr..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aea8d6d1-77c4-4b56-950c-1c1c77fa633b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FID-01.txt</td>\n",
              "      <td>This systematic review provides unique finding...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FID-02.txt</td>\n",
              "      <td>In this review, we describe the application of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FID-03.txt</td>\n",
              "      <td>How to measure similarity between two sequence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FID-04.txt</td>\n",
              "      <td>Text similarity measurement is the basis of na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FID-05.txt</td>\n",
              "      <td>Hoy en día el idioma ha dejado de ser una barr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aea8d6d1-77c4-4b56-950c-1c1c77fa633b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aea8d6d1-77c4-4b56-950c-1c1c77fa633b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aea8d6d1-77c4-4b56-950c-1c1c77fa633b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sus.iloc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpCZ5uKRVJhi",
        "outputId": "d8e89f11-9393-4996-8d9e-1163c88d6f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "name                                              FID-04.txt\n",
              "content    Text similarity measurement is the basis of na...\n",
              "Name: 3, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "  print('---------', i + 1, '---------',)\n",
        "  sus_text = df_sus.iloc[i]['content']\n",
        "  analysis = plagiarism_analysis(sus_text, vector_genuinos, 0.92)\n",
        "  print(analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OwQum7RTrH2",
        "outputId": "d369a3f5-7f81-4b5c-d1d4-28c5d070ad69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 1 ---------\n",
            "Archivo Sospechoso: \n",
            "This systematic review provides unique findings with an updated examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using the PRISMA principles and protocol, 138 articles were identified for a complete examination. Using a priori and reasoned coding, data from the 138 articles were extracted, analyzed and coded. The results of this study show that in 2021 and 2022, publications increased by almost two to three times the number of previous years. With this rapid increase in the number of AIEd HE publications, new trends have emerged. The results show that the research was carried out in six of the seven continents of the world. The trend shifted from the US to China, leading the number of publications. Another new trend is in the affiliation of researchers, as previous studies have shown a lack of researchers from education departments. This has now changed to be the more dominant department. Undergraduate students were the most studied students at 72%. Similar to findings from other studies, language learning was the most common subject domain. This included writing, reading, and vocabulary acquisition. In the survey of who the AIEd was intended for, 72% of the studies were aimed at students, 17% at instructors, and 11% at managers. To answer the overarching question of how AIEd was used in ES, reasoned coding was used. Five usage codes emerged from the data: (1) Assessment/Assessment, (2) Prediction, (3) AI Assistant, (4) Intelligent Tutoring System (ITS), and (5) Student Learning Management. This systematic review revealed gaps in the literature to serve as a springboard for future researchers, including new tools such as Chat GPT.\n",
            "Status: True\n",
            "Similitud: 0.993181\n",
            "Archivo Similar: This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data from the 138 articles were extracted, analyzed, and coded. The findings of this study show that in 2021 and 2022, publications rose nearly two to three times the number of previous years. With this rapid rise in the number of AIEd HE publications, new trends have emerged. The findings show that research was conducted in six of the seven continents of the world. The trend has shifted from the US to China leading in the number of publications. Another new trend is in the researcher affiliation as prior studies showed a lack of researchers from departments of education. This has now changed to be the most dominant department. Undergraduate students were the most studied students at 72%. Similar to the findings of other studies, language learning was the most common subject domain. This included writing, reading, and vocabulary acquisition. In examination of who the AIEd was intended for 72% of the studies focused on students, 17% instructors, and 11% managers. In answering the overarching question of how AIEd was used in HE, grounded coding was used. Five usage codes emerged from the data: (1) Assessment/Evaluation, (2) Predicting, (3) AI Assistant, (4) Intelligent Tutoring System (ITS), and (5) Managing Student Learning. This systematic review revealed gaps in the literature to be used as a springboard for future researchers, including new tools, such as Chat GPT.\n",
            "--------- 2 ---------\n",
            "Archivo Sospechoso: \n",
            "In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.\n",
            "Status: False\n",
            "Similitud: 0.8783421\n",
            "Archivo Similar: This project focuses on the development of a web application that automatically grades the solution to engineering exercises. The input data of each exercise is different for each student in order to reduce plagiarism and increase motivation. Students can access the web app from any device with internet access (computer, laptop, phone,...) at any time. The fact that the exercises are enunciated and evaluated in an individualized way eliminates the possibility for students to share the solutions and divert the profitable collaboration between students towards the learning of the resolution procedure itself. From the professor's perspective, this tool allows an efficient and continuous evaluation of students. Besides, the storage of the data (number of attempts, time required, etc.) provides valuable information both for the self-assessment of the professor and for the analysis of the individualized learning process of each student. The web application is coded in Python, which easily allows the incorporation of additional features according to the needs of professors and students. The web application has already been tested during two academic years in two Spanish universities and for several engineering degrees. Ten professor and more than 2000 students have already benefit from this web application.\n",
            "--------- 3 ---------\n",
            "Archivo Sospechoso: \n",
            "How to measure similarity between two sequences is a key question in information theory and in computer science. Based on Kolmogorov complexity, we have proposed a metric to answer this question and prove it to be universal. A substantial tool is the documents similarity metric applied in fields such as plagiarism detection, problems necessary to capture the semantic, in relation to documents, determining the topic, or structural, or syntactic likeness of texts. To evaluate the proposed method, we construct a comprehensive dataset comprising documents from diverse domains, including academic research papers, news articles, and online forums. We simulate various plagiarism scenarios, such as paraphrasing, sentence reordering, and word substitution, to assess the robustness and effectiveness of our approach. Of different linguistic patterns in plagiarism committing, the taxonomy deep understanding supports, an example is example, texts changing into semantically equivalent but with different organization, texts shortening with concept specification and generalization, and ideas adopting, and of others, key contributions.\n",
            "Status: False\n",
            "Similitud: 0.9031491\n",
            "Archivo Similar: Greedy Texts Similarity Mapping\n",
            "The documents similarity metric is a substantial tool applied in areas such as determining topic in relation to documents, plagiarism detection, or problems necessary to capture the semantic, syntactic, or structural similarity of texts. Evaluated results of the similarity measure depend on the types of word represented and the problem statement and can be time-consuming. In this paper, we present a problem-independent algorithm of the similarity metric greedy texts similarity mapping (GTSM), which is computationally efficient to be applied for large datasets with any preferred word vectorization models. GTSM maps words in two texts based on a decision rule that evaluates word similarity and their importance to the texts. We compare it with the well-known word mover's distance (WMD) algorithm in the k-nearest neighbors text classification problem and find that it leads to similar or better results. In the correlation evaluation task of similarity measures with human-judged scores, we demonstrate its higher correlation scores in comparison with WMD and sentence mover's similarity (SMS) and show that GTSM is a decent alternative for both word-level and sentence-level tasks.\n",
            "--------- 4 ---------\n",
            "Archivo Sospechoso: \n",
            "Text similarity measurement is the basis of natural language processing tasks, which play an important role in information retrieval, automatic question answering, machine translation, dialogue systems, and document matching. This paper systematically combs the research status of similarity measurement, analyzes the advantages and disadvantages of current methods, develops a more comprehensive classification description system of text similarity measurement algorithms, and summarizes the future development direction. With the aim of providing reference for related research and application, the text similarity measurement method is described by two aspects: text distance and text representation. The text distance can be divided into length distance, distribution distance, and semantic distance; text representation is divided into string-based, corpus-based, single-semantic text, multi-semantic text, and graph-structure-based representation. Finally, the development of text similarity is also summarized in the discussion section.\n",
            "Status: False\n",
            "Similitud: 0.88726205\n",
            "Archivo Similar: Document similarity is an important part of Natural Language Processing and is most commonly used for plagiarism-detection and text summarization. Thus, finding the overall most effective document similarity algorithm could have a major positive impact on the field of Natural Language Processing. This report sets out to examine the numerous document similarity algorithms, and determine which ones are the most useful. It addresses the most effective document similarity algorithm by categorizing them into 3 types of document similarity algorithms: statistical algorithms, neural networks, and corpus/knowledge-based algorithms. The most effective algorithms in each category are also compared in our work using a series of benchmark datasets and evaluations that test every possible area that each algorithm could be used in.\n",
            "--------- 5 ---------\n",
            "Archivo Sospechoso: \n",
            "Hoy en día el idioma ha dejado de ser una barrera para plagiar documentos disponibles en Internet. Tras enfoques probabilísticos ya clásicos que no alcanzan buenos resultados con documentos multilingües con paráfrasis.\n",
            "\n",
            "En respuesta a la creciente facilidad para plagiar documentos en Internet, se ha desarrollado un algoritmo computacional que aborda la detección de plagio bilingüe.\n",
            "\n",
            "En este trabajo se construyó un algoritmo computacional para la alineación de textos en la tarea de detección de plagio bilingüe. El método de detección de plagio bilingüe hace uso del servicio de traductores automáticos, con la finalidad de tener los documentos en cuestión en un idioma base, para después aplicar técnicas de plagio monolingüe.\n",
            "Status: True\n",
            "Similitud: 0.9525814\n",
            "Archivo Similar: El plagio es un problema cada vez más significativo, porque en Internet se encuentra disponible enormes recursos de información. Detectar plagio en el código fuente de proyectos desarrollados por estudiantes en las universidades es aún más difícil, debido a que año tras año se generan un sin número de nuevos proyectos académicos y son pocas las herramientas disponibles que permitan analizar grandes volúmenes de líneas de código. Además, la detección de modo automático consume muchos recursos computacionales y tiempos de ejecución. En este artículo se presenta un método mejorado del algoritmo K-Means, el cual permite dividir las líneas de código en grupos y así ahorrar tiempo y operaciones innecesarias en la búsqueda de coincidencias. Los resultados reportan que el algoritmo fue acelerado sin comprometer su propósito original y se ha logrado una aceleración triple en comparación con la implementación básica sin ninguna optimización.\n",
            "--------- 6 ---------\n",
            "Archivo Sospechoso: \n",
            "The short text matching task uses an NLP model to predict the semantic relevance of two texts. It has been used in many fields such as information retrieval, question answering and dialogue systems. This paper will review several state-of-the-art neural network based text matching algorithms in recent years. We aim to provide a quick start guide to beginners on short text matching. The representation based model DSSM is first introduced, which uses a neural network model to represent texts as feature vectors, and the cosine similarity between vectors is regarded as the matching score of texts. Word interaction based models such as DRMM, MatchPyramid and BERT are then introduced, which extract semantic matching features from the similarities of word pairs in two texts to capture more detailed interaction information between texts. We analyze the applicable scenes of each algorithm based on the effectiveness and time complexity, which will help beginners to choose appropriate models for their short text matching applications.\n",
            "Status: False\n",
            "Similitud: 0.9078765\n",
            "Archivo Similar: Paraphrase types have been proposed by researchers as the paraphrasing mechanisms underlying acts of plagiarism. Synonymous substitution, word reordering and insertion/deletion have been identified as some of the common paraphrasing strategies used by plagiarists. However, similarity reports generated by most plagiarism detection systems provide a similarity score and produce matching sections of text with their possible sources. In this research we propose methods to identify two important paraphrase types – synonymous substitution and word reordering in paraphrased, plagiarised sentence pairs. We propose a three staged approach that uses context matching and pretrained word embeddings for identifying synonymous substitution and word reordering. Our proposed approach indicates that the use of Smith Waterman Algorithm for Plagiarism Detection and ConceptNet Numberbatch pretrained word embeddings produces the best performance in terms of F1 scores. This research can be used to complement similarity reports generated by currently available plagiarism detection systems by incorporating methods to identify paraphrase types for plagiarism detection.\n",
            "--------- 7 ---------\n",
            "Archivo Sospechoso: \n",
            "﻿When automatic plagiarism detection is carried out considering a reference corpus, a suspicious text is compared to a set of original documents in order to relate the plagiarised text fragments to their potential source. One of the biggest difficulties in this task is to locate plagiarised fragments that have been modified (by rewording, insertion or deletion, for example) from the source text. However, plagiarism detection is important for authors because of their investigations, researches, articles, among other things.  This paper presents a study of an n-gram-based document comparison method. The method is intended to build a large-scale plagiarism detection system. The work focuses not only on an efficiency of the text similarity extraction but also on the execution performance of the implemented algorithms. We took notice of detection performance, storage requirements and execution time of the proposed approach. The obtained results show the trade-offs between detection quality and computational requirements.\n",
            "Status: True\n",
            "Similitud: 0.9323604\n",
            "Archivo Similar: ﻿This paper presents a study of an n-gram-based document comparison\n",
            "method. The method is intended to build a large-scale plagiarism detection system. The work focuses not only on an efficiency of the text similarity extraction but also on the execution performance of the implemented algorithms. We took notice of detection performance, storage requirements and execution time of the proposed approach. The obtained results show the trade-offs between detection quality and computational requirements. The GPGPU and multi-CPU platforms were considered to implement the algorithms and to achieve good execution speed. The method consists of two main algorithms: a document’s feature extraction and fast text comparison. The winnowing algorithm is used to generate a compressed representation of the analyzed documents. The authors designed and implemented a dedicated test framework for the algorithm. That allowed for the tuning, evaluation, and optimization of the parameters. Well-known metrics (e.g. precision, recall) were used to evaluate detection performance. The authors conducted the tests to determine the performance of the winnowing algorithm for obfuscated and unobfuscated texts for a different window and n-gram size. Also, a simplified version of the text comparison algorithm was proposed and evaluated to reduce the computational\n",
            "complexity of the text comparison process. The paper also presents GPGPU and multi-CPU implementations of the algorithms for different data structures. The implementation speed was tested for different algorithms’ parameters and the size of data. The scalability of the algorithm on multi-CPU platforms was verified. The\n",
            "authors of the paper provide the repository of software tools and programs used to perform the conducted experiments\n",
            "--------- 8 ---------\n",
            "Archivo Sospechoso: \n",
            "We present a systematic investigation of layer-wise BERT activations for general-purpose text representations to understand what linguistic information they capture and how transferable they are across different tasks. Sentence-level embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval, while passage-level embeddings are evaluated on four question-answering (QA) datasets under a learning-to-rank problem setting. Embeddings from the pre-trained BERT model perform poorly in semantic similarity and sentence surface information probing tasks. Fine-tuning BERT on natural language inference data greatly improves the quality of the embeddings. Combining embeddings from different BERT layers can further boost performance. BERT embeddings outperform BM25 baseline significantly on factoid QA datasets at the passage level, but fail to perform better than BM25 on non-factoid datasets. For all QA datasets, there is a gap between embedding-based method and in-domain fine-tuned BERT (we report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.\n",
            "Status: False\n",
            "Similitud: 0.9038294\n",
            "Archivo Similar: Detecting plagiarism involves finding similar items in two different sources. In this article, we propose a novel method for detecting plagiarism that is based on attention mechanism-based long short-term memory (LSTM) and bidirectional encoder representations from transformers (BERT) word embedding, enhanced with optimized differential evolution (DE) method for pre-training and a focal loss function for training. BERT could be included in a downstream task and fine-tuned as a task-specific BERT can be included in a downstream task and fine-tuned as a task-specific structure, while the trained BERT model is capable of detecting various linguistic characteristics. Unbalanced classification is one of the primary issues with plagiarism detection. We suggest a focal loss-based training technique that carefully learns minority class instances to solve this. Another issue that we tackle is the training phase itself, which typically employs gradient-based methods like back-propagation for the learning process and thus suffers from some drawbacks, including sensitivity to initialization. To initiate the BP process, we suggest a novel DE algorithm that makes use of a clustering-based mutation operator. Here, a winning cluster is identified for the current DE population, and a fresh updating method is used to produce potential answers. We evaluate our proposed approach on three benchmark datasets ( MSRP, SNLI, and SemEval2014) and demonstrate that it performs well when compared to both conventional and population-based methods.\n",
            "--------- 9 ---------\n",
            "Archivo Sospechoso: \n",
            "The objective of this document is to evaluate the problems that arise from plagiarism, especially in those worked with texts reused in academic work by students in their academic entities. We focus on preprocessing techniques for Slovak text mappings, such as stopword removal, synonym substitution, lemmatization, and use of the readability index. However, the main objective of this article is to identify, analyze and differentiate an original student activity and compare it with a document suspected of plagiarism and denote its readability, understanding of the subject and similarities between both documents. Finally, already having the results of the experiments and document tests, it was found that the technical combinations and preprocessing methods, to denote the similarities of the students' activities are more effective and if it is desired to analyze and present a similarity with a possible accuracy, the types of plagiarism can be reached and detected with the methods that will be seen in this document later.\n",
            "Status: True\n",
            "Similitud: 0.94274724\n",
            "Archivo Similar: The aim of this paper is to appraise of the problems of plagiarism in students assignments. We focus on pre-processing techniques of Slovak texts assignments such as removing stop words, replacing synonyms, lemmatization, using of readability index. The main goal of this paper is find out if we can identify original student assignment and plagiarism of original student assignment based on their readability. Based on the result of further experimentation, we find which combinations of pre-processing techniques and methods for determining the similarity of students assignments are the most suitable, if we want to detect similarity as exactly as possible and for particular techniques to find out the extent in detection of categorised types of plagiarism.\n",
            "--------- 10 ---------\n",
            "Archivo Sospechoso: \n",
            "Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.\n",
            "Status: True\n",
            "Similitud: 0.9275161\n",
            "Archivo Similar: Binary code similarity detection (BCSD) serves as a basis for a wide spectrum of applications, including software plagiarism, malware classification, and known vulnerability discovery. However, the inference of contextual meanings of a binary is challenging due to the absence of semantic information available in source codes. Recent advances leverage the benefits of a deep learning architecture into a better understanding of underlying code semantics and the advantages of the Siamese architecture into better BCSD. In this paper, we propose BinShot, a BERT-based similarity learning architecture that is highly transferable for effective BCSD. We tackle the problem of detecting code similarity with one-shot learning (a special case of few-shot learning). To this end, we adopt a weighted distance vector with a binary cross entropy as a loss function on top of BERT. With the prototype of BinShot, our experimental results demonstrate the effectiveness, transferability, and practicality of BinShot, which is robust to detecting the similarity of previously unseen functions.\n",
            "--------- 11 ---------\n",
            "Archivo Sospechoso: \n",
            "The RAE defines plagiarism as \"the act of essentially copying someone else's work and making it one's own\". It is basically copying the idea. With the growth of information on the Internet, it has become increasingly easy to plagiarise any work, especially written work. For this reason, reliable tools are needed to detect plagiarism in text. There are two types of text plagiarism detection. Identifying internal and external plagiarism. Internal plagiarism detection is finding passages of text for a given document that may have been taken from some source that knows nothing more than the text itself. External plagiarism detection uses a pair of documents, an original and a suspect document, to determine whether the suspect document is plagiarised from the original. From now on, when discussing the plagiarism detection task, only the external plagiarism detection task will be referred to.\n",
            "There are different classifications of which parts of a text (or spoken language) can be plagiarised.\n",
            "* On the idea: This type of plagiarism is not based on words. A accepts the ideas, thoughts or theories of others without giving them equal weight. This type of plagiarism occurs when the original idea is not made public.\n",
            " * Word for word This is a copy of (an important part of) a sentence. And you can make an exact copy and even make some modifications. If there is no link to the source, it is plagiarism. In addition to literal plagiarism, there is interpretive plagiarism in which both wording and syntax are altered. This approach of interpreting as a form of plagiarism is not entirely correct.\n",
            "* From sources: By including bibliographical references that another author has included in his own work d. However, A does not claim that these references are taken from d. Sometimes, A includes links without even reading them. \n",
            "*Authoritative means that the author of all the material is actually written by someone else. It is not uncommon for students to present other people's reports as their own.\n",
            "Status: True\n",
            "Similitud: 0.9469011\n",
            "Archivo Similar: There are different classifications of which parts of a text (or spoken language) can be plagiarised.\n",
            "* On the idea: This type of plagiarism is not based on words. A accepts the ideas, thoughts or theories of others without giving them equal weight. This type of plagiarism occurs when the original idea is not made public.\n",
            " * Word for word This is a copy of (an important part of) a sentence. And you can make an exact copy and even make some modifications. If there is no link to the source, it is plagiarism. In addition to literal plagiarism, there is interpretive plagiarism in which both wording and syntax are altered. This approach of interpreting as a form of plagiarism is not entirely correct.\n",
            "* From sources: By including bibliographical references that another author has included in his own work d. However, A does not claim that these references are taken from d. Sometimes, A includes links without even reading them. \n",
            "*Authoritative means that the author of all the material is actually written by someone else. It is not uncommon for students to present other people's reports as their own.\n",
            "--------- 12 ---------\n",
            "Archivo Sospechoso: \n",
            "A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user’s query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.\n",
            "Status: False\n",
            "Similitud: 0.9104862\n",
            "Archivo Similar: ﻿Machine learning approaches to source code authorship attribution attempt to find statistical regularities in human-generated source code that can identify the author or authors of that code. This has applications in plagiarism detection, intellectual property infringement, and post-incident forensics in computer security. The introduction of features derived from the Abstract Syntax Tree (AST) of source code has recently set new benchmarks in this area, significantly improving over previous work that relied on easily obfuscatable lexical and format features of program source code. However, these AST-based approaches rely on hand-constructed features derived from such trees, and often include ancillary information such as function and variable names that may be obfuscated or manipulated.\n",
            "\n",
            "\n",
            "In this work, we provide novel contributions to AST-based source code authorship attribution using deep neural networks. We implement Long Short-Term Memory (LSTM) and Bidirectional Long Short-Term Memory (BiLSTM) models to automatically extract relevant features from the AST representation of programmers’ source code. We show that our models can automatically learn efficient representations of AST-based features without needing hand-constructed ancillary information used by previous methods. Our empirical study on multiple datasets with different programming languages shows that our proposed approach achieves the state-of-the-art performance for source code authorship attribution on AST-based features, despite not leveraging information that was previously thought to be required for high-confidence classification.\n",
            "--------- 13 ---------\n",
            "Archivo Sospechoso: \n",
            "Test Collusion (TC) is a form of cheating in which test takers operate in groups to alter normal item responses. CT is becoming increasingly common, especially within large-scale, high-risk exams. However, research on CT detection methods remains sparse. This article proposes a new algorithm for the detection of CT, inspired by the selection of variables within the high-dimensional statistical analysis.\n",
            "This article presents a lemmatization system for the Urdu language, based on a novel dictionary lookup approach. The contributions made through this research are as follows: (1) the development of a large reference corpus for the Urdu language, (2) the exploration of the relationship between part-of-speech labels and the stemmer, and (3) ) the development of standard approaches for an Urdu stemmer. Additionally, we experiment with the impact of Part of Speech (PoS) in our proposed dictionary lookup approach. Empirical results showed that we achieved the best accuracy score of 76.44% through the proposed dictionary lookup approach.\n",
            "Status: True\n",
            "Similitud: 0.9508383\n",
            "Archivo Similar: Lemmatization aims at returning the root form of a word. The lemmatizer is envisioned as a vital instrument that can assist in many Natural Language Processing (NLP) tasks. These tasks include Information Retrieval, Word Sense Disambiguation, Machine Translation, Text Reuse, and Plagiarism Detection. Previous studies in the literature have focused on developing lemmatizers using rule-based approaches for English and other highly-resourced languages. However, there have been no thorough efforts for the development of a lemmatizer for most South Asian languages, specifically Urdu. Urdu is a morphologically rich language with many inflectional and derivational forms. This makes the development of an efficient Urdu lemmatizer a challenging task. A standardized lemmatizer would contribute towards establishing much-needed methodological resources for this low-resourced language, which are required to boost the performance of many Urdu NLP applications. This paper presents a lemmatization system for the Urdu language, based on a novel dictionary lookup approach. The contributions made through this research are the following: (1) the development of a large benchmark corpus for the Urdu language, (2) the exploration of the relationship between parts of speech tags and the lemmatizer, and (3) the development of standard approaches for an Urdu lemmatizer. Furthermore, we experimented with the impact of Part of Speech (PoS) on our proposed dictionary lookup approach. The empirical results showed that we achieved the best accuracy score of 76.44% through the proposed dictionary lookup approach.\n",
            "--------- 14 ---------\n",
            "Archivo Sospechoso: \n",
            "Short text similarity plays an important role in natural language processing (NLP). It has been applied in many fields. Due to the lack of sufficient context in the short text, it is difficult to measure the similarity. The use of semantics similarity to calculate textual similarity has attracted the attention of academia and industry and achieved better results. In this survey, we have conducted a comprehensive and systematic analysis of semantic similarity. We first propose three categories of semantic similarity: corpus-based, knowledge-based, and deep learning (DL)-based. We analyze the pros and cons of representative and novel algorithms in each category. Our analysis also includes the applications of these similarity measurement methods in other areas of NLP. We then evaluate state-of-the-art DL methods on four common datasets, which proved that DL-based can better solve the challenges of the short text similarity, such as sparsity and complexity. Especially, bidirectional encoder representations from transformer model can fully employ scarce information of short texts and semantic information and obtain higher accuracy and F1 value. We finally put forward some future directions.\n",
            "Status: False\n",
            "Similitud: 0.88890076\n",
            "Archivo Similar: Plagiarism is a common problem in the modern age. With the advance of Internet, it is more and more convenient to access other people’s writings or publications. When someone uses the content of a text in an undesirable way, plagiarism may occur. Plagiarism infringes the intellectual property rights, so it is a serious problem nowadays. However, detecting plagiarism effectively is a challenging work. Traditional methods, like vector space model or bag-of-words, are short of providing a good solution due to the incapability of handling the semantics of words satisfactorily. In this paper, we propose a new method for plagiarism detection. We use Word2vec to transform the words into word vectors which are able to reveal the semantic relationship among different words. Through word vectors, words are clustered into concepts. Then documents and their paragraphs are represented in terms of concepts, and plagiarism detection can be done more effectively. A number of experiments are conducted to demonstrate the good performance of our proposed method.\n",
            "--------- 15 ---------\n",
            "Archivo Sospechoso: \n",
            "Plagiarism of digital documents seems a serious problem in today's era. Plagiarism refers to the use of someone's data, language and writing without proper acknowledgment of the original source. Plagiarism of another author's original work is one of the biggest problems in publishing, science, and education. Plagiarism can be of different types. This paper presents a different approach for measuring semantic similarity between words and their meanings. Existing systems are based on the traditional approach. This approach offers the opportunity to tie ethical issues to technical content at the moment students learn it, and to have students engage with these issues repeatedly throughout their degree. However, little is known about the effect of embedded ethics education on students. Therefore, two algorithms are presented: BERT (Bidirectional Encoder Representations from Transformers) which uses a neural network to process the natural language of the texts to capture the context of the words and understand the relationships between them as well as the algorithm USE (Universal Sentence Encoder) which converts sentences into vector representations to perform a semantic analysis.\n",
            "Status: False\n",
            "Similitud: 0.90323603\n",
            "Archivo Similar: Plagiarism is a common problem in the modern age. With the advance of Internet, it is more and more convenient to access other people’s writings or publications. When someone uses the content of a text in an undesirable way, plagiarism may occur. Plagiarism infringes the intellectual property rights, so it is a serious problem nowadays. However, detecting plagiarism effectively is a challenging work. Traditional methods, like vector space model or bag-of-words, are short of providing a good solution due to the incapability of handling the semantics of words satisfactorily. In this paper, we propose a new method for plagiarism detection. We use Word2vec to transform the words into word vectors which are able to reveal the semantic relationship among different words. Through word vectors, words are clustered into concepts. Then documents and their paragraphs are represented in terms of concepts, and plagiarism detection can be done more effectively. A number of experiments are conducted to demonstrate the good performance of our proposed method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones para el Análisis de Similitud"
      ],
      "metadata": {
        "id": "deUepZaUW8y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizer"
      ],
      "metadata": {
        "id": "vuwR-AAbK9le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "dgpHzpQBLHya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para aplicar lematización en los párrafos\n",
        "def lemm_parrafo(parrafo):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  parrafo_lemm = []\n",
        "\n",
        "  for palabra in parrafo: \n",
        "    palabra_lemm = lemmatizer.lemmatize(palabra)\n",
        "    parrafo_lemm.append(palabra_lemm)\n",
        "\n",
        "  parrafo_str = ' '.join(parrafo_lemm)\n",
        "  return parrafo_str"
      ],
      "metadata": {
        "id": "jHuc_EQQLbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "GwwRgjuVJ_iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para aplicar la herramienta de stemming a los párrafos\n",
        "def stemm_parrafo(parrafo):\n",
        "  stemmer = PorterStemmer()\n",
        "  parrafo = parrafo.split()\n",
        "  parrafo_stemm = []\n",
        "\n",
        "  for palabra in parrafo:\n",
        "    palabra_stem = stemmer.stem(palabra)\n",
        "    parrafo_stemm.append(palabra_stem)\n",
        "\n",
        "  parrafo_str = ' '.join(parrafo_stemm)\n",
        "  return parrafo_str"
      ],
      "metadata": {
        "id": "oysmaZS9-0JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOt6dozCML3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones"
      ],
      "metadata": {
        "id": "U0qmbGqGX5eN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73HwrU3ZX9Sl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}